name: "lstm"

data:
  lookback_length: 168
  forecast_horizon: ${horizon}
  batch_size: 64

  past_features:
    - "intraday_wap"          # Target auto-correlation
    - "id_da_spread"          # Relative market value
    - "wind_outturn"          # Realized renewables
    - "solar_outturn"         # Realized renewables
    - "demand_actual"         # Realized load
    - "gen_ccgt"              # Paper: Crucial for "Bridge" price regimes
    - "wind_outturn_delta_1"  # Momentum of generation changes
    - "intraday_wap_rollstd_8" # Recent volatility/choppiness

  known_future_features:
    - "da_price"              # Known benchmark for target hour
    - "demand_da_forecast"    # Scheduled system load
    - "wind_forecast_ng"      # Expected renewable pressure
    - "solar_forecast_ng"     # Expected renewable pressure
    - "hour_sin"              # Diurnal seasonality
    - "hour_cos"              # Diurnal seasonality
    - "is_weekend"            # Industrial load cycle
    - "is_peak_15_18"         # tightness window

spikes:
  # NOTE: This is used by Seq2SeqDataset spike labeling + evaluation thresholds
  upper_threshold: 3.0
  lower_threshold: -3.0

architecture:
  encoder:
    hidden_size: 64
    num_layers: 1
    dropout: 0.0

  outputs:
    quantiles: [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]

training:
  learning_rate: 0.0001
  num_epochs: 10

  # Loss config is partly documentary right now; spike_weight is used in train.py
  loss:
    spike_weight: 1.0

  # This is used (keeps weights between monthly refits in the same run)
  warm_start: true

runtime:
  # torch.device expects "cuda" or "cpu"
  device: "cuda"
