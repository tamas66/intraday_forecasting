name: "lstm"

data:
  lookback_length: 168
  forecast_horizon: ${horizon}
  batch_size: 64

  past_features:
    - intraday_wap
    - da_price
    - wind_outturn
    - solar_outturn
    - demand_actual
    - wind_error
    - solar_error
    - demand_error

  known_future_features:
    - da_price
    - hour_sin
    - hour_cos
    - dow_sin
    - dow_cos
    - is_weekend

spikes:
  # NOTE: This is used by Seq2SeqDataset spike labeling + evaluation thresholds
  upper_threshold: 3.0
  lower_threshold: -3.0

architecture:
  encoder:
    hidden_size: 64
    num_layers: 1
    dropout: 0.0

  outputs:
    quantiles: [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
    spike_probability: true

training:
  learning_rate: 0.0001
  num_epochs: 10

  # Loss config is partly documentary right now; spike_weight is used in train.py
  loss:
    spike_weight: 1.0

  # This is used (keeps weights between monthly refits in the same run)
  warm_start: true

runtime:
  # torch.device expects "cuda" or "cpu"
  device: "cuda"
